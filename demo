We can get out of it, right? At least we will be a little more familiar, I'm hoping, with BigQuery as technology. We'll understand the differences between traditional, how BigQuery differs from traditional data warehouse solutions, and hopefully we'll learn some of the practical techniques for optimizing our queries, which will ultimately drive efficiency and drive the BigQuery costs down. Like I mentioned earlier, I have a lot to cover today. I'll briefly go over agenda items. So we'll start with reviewing, at a high level, BigQuery architecture. We'll dive straight into best practices on the schema design and options we have there, options on loading data into BigQuery, how we can optimize our DML, which is Data Manipulation Language, with your inserts, updates, merges, etc. We'll look at some of the tooling that BigQuery provides out of the box. I apologize. To provide a little bit more visibility into query performance and whatnot, we'll look at some of the best practices for filtering aggregations and joins. Like I mentioned earlier, this will be kind of introductory, and maybe in subsequent sessions, if there is an interest, we can really deep dive into any one of those sessions with examples and demos and all of that. But today is just kind of an overview, and hopefully it will be helpful for everybody. Generally, this targets data engineers and BigQuery admins, but data scientists use BigQuery a lot, and I think 80% of the time, or at least that's what I heard during data prep, it's not only interacting with BigQuery, I understand, but a good portion of it is also kind of writing efficient queries against BigQuery. So I'm hoping that this presentation will be helpful for everybody. So let's dive right in. 

Big Query Architecture:

So BigQuery architecture. I think probably some of you have seen this picture. I know it was presented at many different conferences, and there are a number of YouTube videos showing this image. It shows core components of BigQuery. BigQuery is also called a modern data warehousing solution, and the reason, I think, one of the primary reasons why it's called modern data warehousing solution, because all the components of BigQuery are decoupled, or they can scale independently, right? They're highly scalable, they're durable, and they're independent, right? And sometimes, you know, there could be different zones and different regions, and end users wouldn't even know about that, right? So just kind of quickly touching on different components, right? Part of it is Dremel, or you know, Google calls it Dremel, but essentially, this is like core query engine that Google uses for parsing and executing queries, right? Then there is a distributed storage right here, which Google called Colossus, right? They're not only using it for BigQuery, but essentially using it for everything. It's used for GCS, it's used for other database solutions, it's highly scalable, it's, you know, fast, distributed, and kind of scales independently. They have a very fast petabit network that kind of joins their compute and distributed storage, right? I heard that they use like super secret network appliances and cables, fiber optics and whatnot, that they're kind of a precedent of performance of their network that kind of allows them to interconnect of, you know, storage and compute, not only for BigQuery, but for other offerings that they're using. So it's a very impressive network. The secret sauce of BigQuery, or at least Google calls it secret, lies right here in the middle. It is their in-memory shuffle. So essentially, it's a memory where you're intermediate, where BigQuery stores intermediate results, and we'll double click into that, and we'll go like, there will be a section of a query where I will kind of talk about how a memory shuffle is used. So BigQuery is SQL NC compliant, right? There is a RESTful API, or there is gRCP. There is a web UI, reach web UI, CLI, SDKs for, you know, all the major languages, right? You can stream just data in BigQuery. There are multiple different ways or options, you know, for doing that. One thing that very often kind of overlooked, BigQuery also offers free bulk loading data, and smaller companies, bigger companies, they're not really taking good advantage of it. Instead, they're doing loading with either DML or streaming data, which could be expensive, right? So this is kind of high-level overview of BigQuery decoupled architecture. So, a little bit about managed...

CGP:

**BigQuery Architecture** Let’s get straight to the point with BigQuery best practices. We'll cover how to design efficient schemas, load data, and optimize Data Manipulation Language (DML) operations like inserts, updates, and merges. We'll also check out the built-in tools for monitoring query performance. I'll share some tips on filtering, aggregations, and joins to help make your queries more efficient. This overview should be useful for data engineers and BigQuery admins, but data scientists will also find it valuable. They often spend much of their time preparing data and writing effective queries in BigQuery. If you're interested, we can delve deeper into these topics in future sessions, complete with demos and practical examples. Now, let’s take a closer look at BigQuery architecture. You might have seen this diagram before, which highlights the core components. BigQuery is known as a modern data warehousing solution because each component is decoupled and scales independently. It’s highly scalable, durable, and versatile, operating seamlessly across different regions. Here's a quick breakdown of the components: 1. **Dremel**: This is Google's core query engine for parsing and running queries. 2. **Colossus**: The distributed storage system, not only for BigQuery but also for Google Cloud Storage and other database solutions. It’s scalable, fast, and efficient. 3. **Petabit Network**: The network that connects compute and storage, providing exceptional performance. 4. **In-Memory Shuffle**: The 'secret sauce' that allows BigQuery to handle intermediate results efficiently. BigQuery is SQL ANSI-compliant, offering a REST API, gRPC, web UI, CLI, and SDKs for multiple programming languages. You can stream data or take advantage of free bulk loading, which many companies overlook in favor of more costly streaming or DML loading. That’s a quick overview of BigQuery’s architecture. Let's dig deeper into its managed services now!


Slide 2-4

Do the same thing for below text: So, a little bit about managed storage. Like I mentioned, it's durable, but like when you create your table, right? So, your data or chunks are replicated across multiple zones, across multiple data centers, and if you're in a U.S. multi-region, it's also replicated across multiple regions. And this all happens by default, behind the scenes, so you don't have to do anything. So, it kind of guarantees some of the durability that I mentioned, and helps with gelaster recovery if it happens. And all the writes to BigQuery tables are atomic. It either writes everything or it fails, right? And it at least needs to, before you get a success message, it at least replicates your data between two regions. At least two regions, sometimes it replicates across all regions within a given geography, right? So, it provides this high availability and options. There is also time travel if you, incidentally, if there is a disaster where you deleted your data, incidentally, you can always go back and recover it with a minimal amount of effort. Keep going. I have a lot of content, so I see some raised hands, but I think I will wait until Thursday to take questions, right? I have about 74 slides to cover, and I want to go through as much content as possible. I will try to leave like 15 minutes or so until Thursday, and maybe more for the pre-forum conversation. So, BigQuery columnar storage, right? So, your traditional databases or warehouses, they have record-oriented storage, which is great for transactional type of interactions. BigQuery is optimized for analytical workloads, right? So, the columns are stored, all values in columns, in a given column stored in a file or in a set of files, where you have partitioning and clustering enabled for your tables, which allows efficient use for analytical workloads. If you're selecting certain facts, if you're not selecting star from your queries, then you can get some efficiency there. Also, it uses Google proprietary file format. I believe it's called Capacitor. And since all values in a given column are of the same data type, it allows BigQuery to apply compression. So, you know, once their data is stored on a disk, it's compressed very optimally. And so, I'm going to leave it at that here. Going back to memory shuffle, this is kind of a secret sauce for BigQuery. That's why BigQuery is so fast for analytical workloads, because all the intermediate results are stored in a memory shuffle. You know, we will dive into the life of a query, but essentially, when you select from a table, and as your query progresses its execution, and it generates, you know, it's BigQuery highly parallel, right? There could be thousands of workers working on your query, working on their own portion of the query, and they're all putting their intermediate results into the same memory shuffle. And when time comes to kind of stitch all the results together, it's just a matter of, it's like working with pointers. It's all in the same place, right? There is no further shuffling or further movement via network or anything, so it's all on the same shuffle. Next slide will be about BigQuery slots. And I know somebody asked before, like, what's BigQuery slot, right? So BigQuery slots, that's how BigQuery measures its analytics, if you will. So I see BigQuery slots as workers, but they're comprised of CPU, RAM, and network. And slots are different than memory shuffle, right? But there is a correlation, right? When you buy capacity from BigQuery, the more slots you have, the more memory shuffle they're allocating for your projects or organizations. So there is a tight correlation. And there are different types of shuffles, I should mention, right? BigQuery can shuffle onto network, that is, when there is no intermediate results, so the data is shuffled to the network. Or it can shuffle to a disk. When you run out of memory shuffle space, you don't have sufficient slots, it's going to start shuffling onto a disk, which is much, much slower. And when that happens, it's usually there is something wrong with your query or provision capacity. You probably have slot contentions, et cetera, et cetera. But we'll cover it more later.

CPG4:

**Managed Storage and BigQuery** Let's chat a bit about managed storage in BigQuery. It’s built to be durable. When you create a table, your data is automatically replicated across different zones and data centers. If you’re in a U.S. multi-region, it’s also replicated across multiple regions, giving you extra protection. This replication ensures data durability and recovery behind the scenes, so you don’t need to worry about it. BigQuery writes are atomic—either all data is written, or none is. Before confirming success, your data is replicated across at least two regions, and sometimes even more within a geographic area. This design ensures high availability and resilience. Another great feature is time travel. If you accidentally delete your data, you can easily recover it with minimal effort. Now, let's touch on BigQuery's columnar storage. Unlike traditional databases that store data in a record-oriented way, BigQuery is optimized for analytics. It stores all values from the same column together, which, combined with partitioning and clustering, makes analytical workloads much faster. You can efficiently query specific facts rather than always querying entire rows. BigQuery uses a proprietary file format called Capacitor that enables effective compression, which further optimizes data storage. Moving on to the in-memory shuffle feature: this is a big reason why BigQuery is so quick for analytics. As your query runs, thousands of parallel workers can handle intermediate results using a shared memory shuffle. When the final results are needed, everything is already in place, without the need to shuffle further across networks. Finally, BigQuery slots are worth mentioning. Slots are the resource units that measure analytics capacity in BigQuery. Each slot includes CPU, RAM, and network components. More slots mean more memory shuffle allocation, which speeds up processing. If you’re shuffling data to disk due to insufficient slots, it's slower and a sign that your query or resource provisioning may need adjustment. Let’s dive deeper into the next slide and explore BigQuery slots further!


Slide 5-

BigQuery first schedule, I think it's important to cover, to kind of deal with slot contentions, right? Because, you know, there could be a finite number of slots allocated to your project. And this example gives, like, let's say you have 3,000 slots and you start running your query. One thing I should mention, BigQuery execution is very dynamic. So when you, like, run your query, you know, Query Optimizer figures out ultimate plan to run your query based on your environment, how many slots you have available, what else is working on your environment. And it's going to use all the capacity it needs to run the query most efficiently. But it's aware, real-time, of your environment. And after each step, it yields backward checks saying, is there anybody else doing anything with an environment? And let's say during your query execution, somebody else triggered another query. It's going to fairly divide your capacity. Essentially, it's going to take some of the originally provisioned slots for your first query and reallocate them for a second. So the more queries you run, there is a limit how many concurrent queries you can run in a given project, right? So the reason why I'm kind of sharing this slide, I think it's important to understand there is a fair scheduler in a BigQuery and sometimes running more queries in parallel is not necessarily will yield greater performance. Some people say, hey, we're just going to use Dreadpool and we're going to execute like 1,000 of them. There will be all kinds of contentions and queries are going to be running much slower. So it's very important to kind of be cognizant of your environment, seeing how much capacity you have and kind of reasonably schedule your workload so that the high parallelism is not necessarily a good thing in BigQuery. BigQuery slot reservations. So as many of you remember, initially we weren't on the reservations, but we were on-demand pricing. On-demand pricing, Google loves it because they make a lot of money on it. Essentially, you're paying per query, per bytes process, which could be very expensive. And then it's kind of really impeding your progress because you're always thinking, hey, if I run this query, it's going to cost me X number of dollars and people are hesitant to run queries because, you know, somebody is watching costs going up. So there is this concept of reservations and right now, it's changed recently, so there are multiple tiers. There is like standard tier and premium tier, but the gist of it, you buy your fixed capacity and you can commit to it. I think the lowest is hour or minutes, maybe. I can't remember the lowest, but essentially you can buy commitment, you can buy your monthly commitment, annual commitments, or even like I think you can buy commitment for minutes or seconds. I don't remember the lowest granularity here, but at least with the commitments, your cost is predictable and it's much cheaper than if you would be paying for on-demand pricing. In one of the cost containment efforts, I was kind of giving an example of on-demand pricing versus if you buy commitment, it's significantly less. So the kind of hierarchy with commitments, you buy your commitment, you create your reservations. Generally, people create reservations either per teams or per tenancies. In our case, even there will sometimes go more granular saying within a given tenancy, there are different types of workloads. There are critical workloads, there are less critical workloads, and then there are assigned projects to those reservations. And as we're moving into compute projects, I think we'll have some flexibility of kind of playing with the reservations and right-sizing our projects in the reservations to take advantage of them. And assignments are quick. There is an example, like if you know you have nightly jobs that are running during the night, you can reallocate capacity to kind of boost up some performance of those jobs. During the day, your data scientists are heavily doing experimentation and out-querying, and you can take that capacity and reallocate that back to sandboxes where people are actually running ad hoc queries. It can be scheduled, it can be dynamic, and there's also auto-scaling element, which Google suggests. I personally, it's a good thing that ensure that you don't run into slot contentions, but you need to be very careful. When I look at the auto-scaling, it's like over-scales. For instance, your queries just need a little bit of capacity, and it allocates like three-fold. And for auto-scaling, you pay full price for that kind of scaled-up capacity. That's the most kind of overview of our architecture. Hopefully this was helpful just to kind of understand all the core pieces of BigQuery.


CPG4:


**BigQuery Fair Scheduler and Slot Reservations** Let's talk about the BigQuery fair scheduler. This is crucial for managing slot contention because each project has a limited number of slots available. For instance, if you have 3,000 slots and start running a query, BigQuery dynamically optimizes its execution plan based on your environment and slot availability. It adjusts in real time, checking after each step to see if other queries are running. If another query starts, the scheduler will fairly redistribute some slots from the first query to the second. It's important to understand that running more queries in parallel doesn’t always lead to better performance. Some people assume they can run hundreds of queries at once with a thread pool, but this often leads to contention and slows things down. Be mindful of your environment, capacity, and workload schedule to avoid over-parallelization. Moving on to slot reservations: initially, BigQuery operated with on-demand pricing, which charges per query based on processed bytes and can become expensive quickly. With reservations, you buy fixed capacity on a monthly, annual, or even minute-by-minute basis. Committing to a specific capacity is not only predictable but also cheaper than on-demand pricing. You can organize reservations by team or tenant, or even differentiate between critical and non-critical workloads. Assign projects to these reservations to align them with your compute needs. This flexibility allows you to adjust capacity for nightly jobs and then reallocate it to sandbox environments during the day when data scientists are experimenting. While auto-scaling can help prevent slot contention, be cautious as it might over-allocate resources and increase costs unnecessarily. That’s a quick overview of the BigQuery architecture, fair scheduler, and reservations. I hope this clarifies the core features and helps you understand how to manage your BigQuery workloads more effectively.


Slide 19- partion and cluster:

I'll jump straight into schema design, and I think core of it, well, let's talk about this slide, right? Google will tell you to not optimize prematurely, right? Postpone optimization, you know, get your data migrated to BigQuery, and just try running it as it is, and if it performs, just leave it alone, right? My suggestion is just kind of, use rationale, right? Really think about this, like if you're migrating large tables, and you need to join them with another large tables, optimize. Optimize early enough, like think about your schema design, because post-on-put implications can be very, very, very high and painful, right? For smaller tables, if we're talking under one terabyte, I agree with the statement, kind of move your data under one terabyte, do not worry about, try to denormalize it, or apply partitioning, or clustering, a thing that might not be necessary, but for a larger table, definitely think about your schema design. Where are we at? So, partitioning, right, essentially Google gives you two main levers for optimizing your schema, this is being partitioning, and clustering. So partitioning is, basically, think about partitioning as a smaller table, so if you have a big table, you can create partition, and we'll look at different types of partition, where you're essentially providing efficiency by creating smaller chunks that are physically organized based on the field that you're partitioning on, so this can be a big field, or an integer field, and there are a lot of benefits, like for instance, that there is a cold storage, so I mentioned BigQuery storage is durable and distributed, but in BigQuery, also, it's cost efficient, right, for instance, you partition your table, you load your data in a table, and let's say you're not touching that data for 90 days, and not touching, you can query that data, but you're not mutating, you're not changing your data, so you're automatically getting a 50% discount on that, and think about your data is compressed using Google compression, the capacitor file format that I mentioned, which is at least 5x of original size, or more, for certain data types, or for certain tables, so you really, and we're paying for physical bytes after compression, right, you can get more benefits for storing data in BigQuery and partitioning tables than, you know, some people say, hey, we'll use GCS because it's less expensive, it's not necessarily true, so it has to be assessed, but benefits of partitions, it really comes when you're querying data, right, so if you partition your table, right, and the general guideline, partition on columns that are most frequently used for filtering, right, so if you're filtering on dates, right, that's how you want to partition your table, and BigQuery will do partition pruning, instead of doing full table scan, it will only scan those partitions that, you know, where your data resides. One thing I want to mention, there is also a concept, and I think it's a lot less popular now, but I still see a lot of it at CVS, table sharding, where people, and I think there will be a slide on table sharding, where people, instead of taking advantage of Google partitioning, they'll shard their tables, essentially, you can add prefix to your table, and then you can query multiple tables within a single query, but it's considered anti-pattern, not necessary, and it, you know, creates additional overhead, also, if your tables are partitioned, you can achieve faster joins, right, if your tables are partitioned, and you're joining on partition keys, then your joins will perform faster. Partition types, like I mentioned already, this can be a date, or it can be an integer, there are a number of posts out there saying, like, you can partition on any field, and there are, like, tricks how you can convert string fields to integers, and dates, and hashes, and things like that, but those are two main data types that you can partition on, integer and dates. There is this limit, but it's a soft limit, so you can have up to 4,000 partitions per table, so if you want to go, like, with a big partition, you can go very granular, it doesn't need to be date, it can go as granular as hourly partition, and I would, you know, I would really look if that's necessary, but in some cases, yes, like, if you have a data that arrives very frequently, you probably want to implement very granular partitions, so that you're touching only that partition, so something to keep in mind, but I know many customers run into this 4,000 partitions per table limit, but it's a soft limit, we can contact Google, and they can double it, I don't think they can triple it, but at least it can be doubled, something to be aware of. The next one, so partition, as you remember, I said that this is how data is physically segmented on a disk, so essentially it will be a separate file, so if you partition on dates, or monthly partition, your January will be in one file, February will be in a separate file. Clustering, on the other hand, it's like how data is organized, think about this as, like, maybe fragmentation, for lack of better terms, but essentially source the data, so like values will be very close together, Google making a lot of effort to make clustering even better, to improve their clustering strategy, in some cases they're actually saying that clustering can yield better performance than if your table is partitioned, because when tables partition, there's additional metadata, and if you have a lot of partitions, there is this overhead of reading that metadata, essentially file header, saying like, hey, what are the range of the values in this file, et cetera, et cetera, while clustering works very similar to indexes, so there are situations, especially on the tables that are not that large, well, still could be in multiple terabytes, but clustering can yield faster performance. What it does not provide, though, it provides efficient query pruning, but you cannot take advantage of a long-term storage cost, right, so there is no debt with clustering, something to be aware of.


CGp4:


**Schema Design in BigQuery** Let’s dive into schema design. Google advises against premature optimization—just migrate your data to BigQuery and run it as-is. If performance is satisfactory, leave it alone. But use your judgment: if you're moving large tables that need to join with other large tables, optimize early by thinking carefully about schema design. Delaying optimization could lead to significant headaches later on. For tables under 1 TB, you might not need to worry too much about denormalizing, partitioning, or clustering. But for larger tables, planning the schema design is crucial. **Partitioning and Clustering** Google offers two main tools to optimize your schema: partitioning and clustering. **Partitioning** Partitioning breaks a large table into smaller chunks based on a field like date or integer. It improves efficiency by organizing data into physical segments on disk. You can also save on storage costs: if you don't change the data for 90 days, you'll get a 50% discount. Since BigQuery compresses data with its proprietary Capacitor format, you're charged based on physical bytes after compression. To get the most out of partitions: - **Partition on frequently filtered columns:** This lets BigQuery prune partitions during queries, scanning only the necessary segments. - **Avoid table sharding:** Sharding, where you manually prefix table names to query multiple tables at once, is considered an anti-pattern and creates unnecessary overhead. BigQuery lets you partition up to 4,000 segments per table (a soft limit that Google can double on request). You can partition by hour, day, or another granular level if your data frequently updates. **Clustering** Clustering organizes similar values close together, allowing faster data retrieval. In some cases, clustering can be faster than partitioning due to the overhead of reading partition metadata. However, unlike partitioning, clustering doesn't offer storage discounts for long-term data. To summarize, partitioning and clustering help you efficiently organize your data, but each has specific use cases. Choose the right method based on your data size, query patterns, and cost considerations.


Slide 20_


Sometimes you can get even better results with partitioning and clustering, especially if you need to use joints and if you're using your clustered keys in your joints, in your partition key, in your work loss. One thing about partitioning and clustering and what we're observing is part of the cost containment. A lot of people don't partition or cluster their tables. Some teams, they would partition and cluster their tables, but not based on the usage pattern. The general guideline, you need to know how your table is used and there are ways of actually doing analytics on it. You can look at metadata and say, okay, I have this large table. How are people using it? And based on those usage patterns, you can kind of decide what your partitioning and clustering scheme needs to be. It needs to align with the usage pattern to really take full advantage of the partitioning and clustering. Are we done on time? We're good. I need to move faster. Table charging. I already mentioned that and here's an example of table charging, right? You have some table and you're pending dates. I think it's very common in EDP. They're doing daily refreshes and they're charging their tables. Personally, I think it's necessary. It can be easily achieved with less overhead, with less maintenance, with just partitioned tables. Again, as an example, at Home Depot, it was very common for large tables to have underscore history and underscore current, right? Nothing that can be easily achieved with partition because you're running issues where your schemas are drifting and things of that sort, so not a recommendation. Materialized view. This is one of my favorite features in BigQuery. When we talk about materialized views, I'll encourage people to read up on it. Previous contract, somebody asked us that there needs sub-second response time from BigQuery on large dataset to power their dashboard. We're like, okay, BigQuery is not that low-latency database, but let's see what we can do. We took advantage of materialized views. We didn't reach sub-second performance, but we got very close. Under five seconds, I think, our data was returning from a very large table. It allows, for instance, if you need to do some aggregations on the data and you have some data maybe arriving, materialized use in BigQuery is not the same as materialized views in Oracle where you need to validate or refresh them. BigQuery takes care of all of that for you. There are different mechanisms to either auto-refresh. But in BigQuery, materialized use data is never stale, so it's always real-time. As your data arrives, it automatically computes what's not yet materialized, but then on certain cadences or certain triggers, it triggers materialization. It's very powerful features if you need to do some aggregations, like real-time aggregations. The next big thing is the BI engine, but the BI engine could be very expensive. It's a separate capacity provisioning model and whatnot. So for anything that requires real-time refreshes or if you're running aggregations frequently on some data, materialized views could be the thing to support that. Materialized views are still evolving. It's a relatively new feature. I'm talking about the last two or three years. I can't remember exactly when they were introduced. So not all DMLs are supported. You cannot write materialized or immutable. You cannot write, update data within materialized views. Certain functions are not compatible with materialized views. Most of the joins are allowed, but there are some limitations, and I'm not going to go through those. They're available in documentation. Aggregate functions generally work, but some of the more complex analytical functions, like windowing functions and whatnot, might cause some issues. So use with caution, but it's one of my favorite features of BigQuery when you need to boost some performance.

CGP4:

**Optimizing with Partitioning, Clustering, and Materialized Views** Combining partitioning and clustering can lead to even better performance, especially when joining tables using clustered or partition keys. However, many teams don’t leverage these features based on actual usage patterns, which results in missed opportunities for cost savings and efficiency. To get the most out of partitioning and clustering: 1. **Analyze Usage Patterns**: Review your table metadata to see how people are querying the data. Then, design your partitioning and clustering strategies based on these patterns. 2. **Avoid Table Sharding**: Table sharding can be cumbersome and less efficient than partitioned tables, so it's best to use partitioning whenever possible. **Materialized Views** Materialized views are a powerful feature in BigQuery that help with performance, especially for aggregations. They’re not like traditional materialized views in other databases where you must manually refresh them. In BigQuery, the data is always up-to-date, and Google handles the refreshes. Advantages: - **Real-time Aggregations**: Materialized views automatically aggregate new data as it arrives and update based on specific triggers, giving you near real-time results. - **Fast Querying**: This is great for dashboards that require quick aggregations, though you may not always reach sub-second response times. Limitations: - **No DML Support**: Materialized views are read-only, so you can't update data directly within them. - **Complex Functions**: Some complex functions like windowing aren’t supported. **BI Engine** Another option for enhancing query speed is the BI Engine, which offers separate provisioning for high-performance analytics. However, it can be expensive, so consider using materialized views as a cost-effective alternative. In summary, materialized views are a relatively new feature, and while there are some limitations, they’re a fantastic tool for speeding up frequent aggregations and queries. They should be used with caution, but when applied properly, they can significantly boost performance.


Slide 21-loading data:

Next, loading data into BigQuery. There are multiple ways of loading data. BigQuery already mentioned one of them, but I'm trying to stress this out, and a lot of organizations make this mistake. During the link and ship processes with traditional data warehouses, it's usually ETL methodology, where you actually, during your pipeline, you look up values, you transform your data, and then you load your clean data into your database. Even sometimes if there's a staging, you're still doing some of the data manipulation within your ETL tool. BigQuery recommends, you know, BigQuery is highly scalable. What they're suggesting is get data into BigQuery in the raw format. Do not do any data manipulation within your tool. Do not, like, spend waste or however you want to put money on doing this in memory or within, you know, some people pay for IBM data stage, or I can't remember, and massive clusters of ETL. Say, like, get raw data in BigQuery and then use just the ML language or other techniques like we will cover in a minute to transform your data into, like, reporting or analytical format. So ELT is preferred. Extract, load, then transform, as opposed to extract, transform, then load. But it's still a problem. People spend a lot of money on doing extract, transform, and load. So some of the options for loading data into BigQuery. You can do it with DDLs. Here's the example of DDL, where you're creating a table as select from other two tables. And the recommendation is, you know, as fewer statements as possible. If you need to load data from multiple tables, combine them with union all, but try to, you know, limit the number of statements because BigQuery can scale. So you don't need to, like, parallelize it yourself and create a table and then have another call statement. If you do that, BigQuery optimizer will figure out how to load your data in the most efficient way. We have loading with DML, merge statements, also my favorite, and now syntax. Some people kind of staying away from it, but in the merge statements, it's a single statement and can perform all three GML operations. You can insert, update, and delete data all within a single statement. We'll talk, if I don't run out of time, about... I think I'll not go through the entire presentation. I'll stop at some point because I wanted to leave a little bit of time for questions, and if there wouldn't be any questions, then we can continue. But I'll share this deck. There's some good nuggets in here. You can review it yourself. A lot of it is self-explanatory, like I was offering earlier. We can do a deep dive on some of those sections. So there are some alternatives. Instead of three statements, you can do one query job or you can do one GDL statement. We talk about partitioning, right? I've seen a lot where people trying to load data into their table and doing the full table scan to just insert the last month worth of data, the last hour worth of data. There is this concept with the partition decorators where you can just overwrite one single partition. So if you know that you press data daily, there is no reason for you to even write a merge statement. Some merge statements that can scan the full table scan before they're inserted just on the tail end of your data, which is a very big worth of data, could be very inefficient. But these partition decorators provide very efficient ways of just overwriting one single partition. You can overwrite last hour, last month, or last year. So it's very efficient without touching the entire table, just working with a specific partition. In this example, the difference between this and that, you can either load this from federated table, which is essentially an external bucket, or you can load from your row or read from your external or staging table. My favorite part, and it's not very... A lot of big players in BI space and ETL space have adopted this BigQuery Storage API. This is a new, Google-recommended, modern way. It compresses data. You use Arrow, I believe, format. You can use another format. I forget the extension of it. But this is Google-recommended way, not only for reading data out of BigQuery, if you need high-throughput, like all the BI tools, if you think Tableau, Altra, Sutter, they're adopted using BigQuery Storage API. The BigQuery Storage API doesn't go through this normal process of scheduling your jobs and executing your job through Shuffle and all of that. It interacts directly with... It's Dremel, right? It's pulling data directly from BigQuery Storage. And when we talk about BigQuery Storage API for writes, it also writes directly to storage, bypasses all the BigQuery Slab content, passable Slab contentions, etc. It's very efficient. It's properly coded. It's highly scalable. When we were experimenting with throughput on this, the results were astonishing. I'm like, how is it even possible, right? So you can get very high throughputs of writing data. And BigQuery Storage API supports real-time micro-bashing and batching. So there's clients available for it. But that's what Google recommended. I think that there were use cases. I think we were actually thinking about this. When we were parsing sequentially a data frame, I think the better solution would have been to adopt BigQuery Storage API. Again, something worth looking into if we're thinking that we need a high throughput for either reading data out of BigQuery or writing data into BigQuery. We have 15 minutes left. I have more slides on DML optimization. Life of query is very interesting, but maybe I can save them for next time. I want to kind of stop right here and see if people have...


CGP4:

**Loading Data into BigQuery** Let’s talk about loading data into BigQuery. There are several ways to approach this, and it’s important to avoid common pitfalls that many organizations face. In traditional data warehouses, the ETL (Extract, Transform, Load) method involves transforming data before loading it into the database. BigQuery, however, is highly scalable and recommends loading data first in its raw format, then transforming it using SQL or other tools once it's in BigQuery. This is the ELT approach: Extract, Load, then Transform. ### Best Practices for Loading Data 1. **Use SQL DDLs**: Create tables using SQL Data Definition Language (DDL) commands like `CREATE TABLE AS SELECT` to load data efficiently. If you need to load data from multiple tables, use `UNION ALL` to minimize the number of statements. 2. **DML Merge Statements**: Merge statements can be your friend, as they combine insert, update, and delete operations into a single statement. This simplifies data modification. 3. **Partition Decorators**: Instead of running a full table scan just to insert the latest data, use partition decorators to overwrite specific partitions. You can efficiently overwrite the last hour, day, or month without touching the entire table. 4. **BigQuery Storage API**: For high throughput, consider using the BigQuery Storage API. It allows direct reading and writing of data, bypassing job scheduling and shuffling processes. This approach is used by major BI and ETL tools like Tableau and Alteryx. With this API, you can read and write data directly to storage, achieving incredible throughput for both micro-batching and larger batches. ### Closing Thoughts I hope this gives you a good starting point on loading data into BigQuery efficiently. There are additional best practices and optimization strategies, particularly around DML operations. For now, feel free to review this information, and let's save some of the more advanced strategies for another time. Let me know if you have any questions!


Slide 30-

DML is your data manipulation language, so it adds your inserts, updates, deletes, and whatnot. So, optimizing merges, like I mentioned before, merges are a preference because you can combine multiple DML operations into a single merge statement that can do updates, deletes, and inserts. So, the key with merges is to avoid full table scans. First, you need to make sure that your target table is partitioned and clustered. Second, that you use those partition and cluster keys in your match clause. And the third, I think it's very big, and I think it applies not only to merges, but to BigQuery in general. BigQuery cannot do a partition pruning based on dynamic predicates. Dynamic predicates, essentially, if you're selecting, like, in your in clause, like using correlated queries, or there is some logic like that, or you're, like, in your word clause, you're determining the range mean and max, like, it cannot process that, and it will lead into a full table scan, very inefficient queries. And on large tables, BigQuery, like I mentioned before, is optimized for analytical workloads. It's not optimized. There are numerous articles saying, like, I'm limiting DMLs for BigQuery, et cetera, et cetera, that the gist of it is BigQuery is not optimized because it's columnar storage. It's not optimized for DMLs. It's optimized for reads. So that's why you need to do a little bit of gymnastics to get your DMLs, you know, working as efficiently as possible. And the general recommendation is to use static predicates. So if you know your keys, do not try to derive them from, like, select statements and things like that. Explicitly provide them. Another trick, and I think this will be on the next slide, you can store those values that you're mashing on in a variable as opposed to, like, running select or computation of some sort, and then this variable will be considered as static and it can get some performance. I think this is an example of it. Like, during data ingestion time, generally you're working with a, you know, you're loading data into certain partitions or a certain time range, and very often people will do something like, hey, my data is between those dates. Well, this syntax will result in a full table scan, potentially, and it poops inefficient queries, right? The recommendation is kind of store that in a variable, and then it becomes your static variable and will result in very efficient queries. Delete operation, the same thing. This results in full table scan. If you do something very similar, but you store it in a variable, then it becomes static condition, and it's going to be much more efficient. Life of a query. So, again, going back to basics, so we're going to kind of a little dissect this query. With the, you know, like I mentioned, in BigQuery, when you execute, when you run your query, it generates a query plan, and there are generally multiple stages. Stage one generally involves inputting data or reading that data in parallel from Dremel, from distributed storage, and in our case, we have count, and the discount will be done in a single slot, and the results will be returned to Ambassador. The way it kind of, with a little more details, it's visualized. You read data from your file system. There could be multiples. It can be depending on how big is your data, if it's splitable, not splitable. If you're reading from GCS, for instance, you know, BigQuery is going to determine, like, the level of parallelism it requires, and it's going to scale that, it's going to put your intermediate results onto a shuffle, and then depending on your operation, like in this specific case, the count, because we don't have any group bytes or anything, it's going to be done by one single slot, and then it will be written to a file system and presented to you in the UI. If you look at this plan in the BigQuery execution plan, right, that essentially kind of shows those two stages, right? We're kind of reading from bottom to top, right? So you have your input stage, which is right here. It's selecting from your table. It's applying some filtering, and it's doing partial. So each slot will perform its own count, in other words, right? And then this last step is going to just sum up all the counts for them from each slot, right? But results of it are going into an intermediate layer in memory shuffle, and you can, you know, view execution plan right in your GUI. So we have some additional stats here. We have CPU, which is your compute, right? And we have read and write stats. One thing to keep attention here, during query troubleshooting, if this ratio is too high, like records read and records written, if records written are either higher than records read, there's probably some sort of skews happening. If they're the same, that means there's no pruning happening. That means, you know, you need to evaluate either your word clause or to see if your table is partitioned or clustered. Another nice feature that BigQuery has is cache preference. I think it's on by default. I don't remember, but I think it's on by default. So if you run query within BigQuery, your results are cached for 24 hours. So if you need to go back to those results, you run exactly the same query. Your syntax is unchanged. The number line data is not refreshed yet. It's going to pull your data from cache, which, you know, process essentially is free operation. It's not billing you for either slots or storage or anything. And the cache leaves there for 24 hours, like I mentioned. How do you optimize query, right? So essentially, you know, things that are considered work for query is how many bytes are read, shuffle, and materialization. How much do you write out? And I think this is very important. I've seen, like, people just making copies of very large tables. Maybe there is a reason, but very often I think it is not necessary, right? I think the key is to kind of minimize how many bytes you output to shuffle. And kind of the rule of thumb is try to filter your data as early as possible. BigQuery is still maturing. Some of the traditional databases, I think they're smarter in that respect that they can determine. But, hey, you're, you know, joining big table and small table, and you're probably trying to filter your big table with a smaller table. BigQuery is not smart in that regard. It's going to do a full table scan of one another, and then it will try to limit your results. What's recommended is providing word clause as explicitly and as early as possible. When you look at query plan, I think there are three main things that you want to look at. Like if your query is efficient or not, right? So if you see significant difference between your average max time and average and max time, that means there is some skew going on, either a bad join or data skew or duplication in your data. I wish I can show that, but I'm not ready for that demo, and we're almost out of time. I think this will be our last section, and probably we'll pick up if there is interest during one of the future launch and learns. Another big indicator, if you look at your stages in your execution plan, and you see that most of the time it spends somewhere in the middle, that's also a red flag. That means your query is not efficient, your schema is not efficient, something is off, and it requires further double-clicking. And again, most time is spent on CPU tasks, like compute tasks, that means there probably could be some inefficiency, or you can. Very recently we ran, and I think we've been remediating that, CVS using Bolt, which is an external API, and they've created some UDF functions to fetch encryption keys and whatnot, and then they use those UDF functions inline in their queries, so essentially it invokes functions for every row it processes, and if you're selecting from a table with millions of rows, there's millions of those calls, so we've seen this spike in CPU, and we were coming up with some remediation plans for that. I'm going to probably stop and have seven more minutes.


CGP4

**Optimizing Data Manipulation Language (DML) Operations in BigQuery** Data Manipulation Language (DML) encompasses your inserts, updates, and deletes. Here’s how you can optimize merges, which allow you to combine multiple DML operations into a single statement: 1. **Avoid Full Table Scans**: Make sure your target table is partitioned and clustered. Use these partition and cluster keys in the match clause. 2. **Static Predicates**: Use static predicates in your `WHERE` clause rather than dynamic predicates like correlated subqueries or ranges. Dynamic predicates result in inefficient, full table scans. 3. **Use Variables**: Store key values in variables instead of computing them dynamically. This will treat them as static and improve query performance. ### Partition Decorators Instead of scanning the entire table to insert recent data, use partition decorators to overwrite specific partitions directly. This lets you efficiently update just the last day or month without affecting the whole table. ### Query Execution Plan A BigQuery query generates a plan with multiple stages. The initial stage usually involves reading data from storage in parallel. Intermediate results are written to memory shuffle, where they’re aggregated in the final stage. ### Optimizing Queries 1. **Review Execution Plan**: The plan provides useful metrics like CPU and read/write statistics. If the ratio of records read to written is high, you may have inefficient queries that need further pruning or partitioning. 2. **Caching**: By default, BigQuery caches results for 24 hours, letting you rerun the same query without reprocessing the data, which reduces costs. 3. **Efficient Query Practices**: - Minimize bytes written to the shuffle. - Apply filters as early as possible. - Avoid unnecessary copies of large tables. ### Indicators of Inefficient Queries 1. **Average vs. Max Execution Time**: If there’s a big difference between the two, there may be data skew, inefficient joins, or duplicate data. 2. **Execution Plan Stages**: If most time is spent on intermediate stages, your schema or query needs optimization. 3. **High CPU Usage**: Inline User-Defined Functions (UDFs) can cause a spike in CPU usage if used extensively. These tips can help you make your DML operations more efficient and your queries more effective.

MPP
Query pruning
Struct in BQ
Bq shuffle
Data skew
Query Plan



